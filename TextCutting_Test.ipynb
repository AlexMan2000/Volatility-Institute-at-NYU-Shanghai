{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.utils.text_preprocessing import TextPreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取train_data中\n",
      "加载已有数据\n",
      "获取train_tokenizer中\n",
      "加载预训练Tokenizer\n",
      "获取embedding_matrix中\n",
      "加载预训练Embedding Matrix\n"
     ]
    }
   ],
   "source": [
    "textProcessor = TextPreprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = open(\"./Data/speech.txt\",\"r\",encoding=\"utf-8\")\n",
    "def extract_dictionary(file):\n",
    "    print(file)\n",
    "    if file:\n",
    "        dictionary_list = []\n",
    "        assert file != None\n",
    "        txt = file.read()\n",
    "        assert txt != None\n",
    "        dictionary_txt = re.findall(\"{'article'.*?'}\", txt, re.S)\n",
    "        for sub_txt in dictionary_txt:\n",
    "            tmp_list = re.findall(r\"\\'(?P<key>.*?)\\':\\s['|[]?(?P<value>.*)['|]]?\", sub_txt)\n",
    "            tmp_dict = dict(tmp_list)\n",
    "            tmp_dict[\"date\"] = re.sub(\"'\", \"\", tmp_dict[\"date\"])\n",
    "            dictionary_list.append(tmp_dict)\n",
    "        return dictionary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='./Data/speech.txt' mode='r' encoding='utf-8'>\n"
     ]
    }
   ],
   "source": [
    "results = extract_dictionary(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': '新华社北京8月30日电国家主席习近平8月30日同厄瓜多尔总统拉索通电话。习近平指出，新冠肺炎疫情发生以来，中厄双方相互支持、同舟共济，体现了两国人民的深情厚谊。中方赞赏厄方高度重视发展对华关系。面对世界百年变局和世纪疫情交织的复杂局面，中厄作为全面战略伙伴，要从战略高度和长远角度看待和发展两国关系，继续相互支持、协调合作，让中厄传统友好历久弥新，使中厄关系成为发展中国家合作标杆。中方愿继续为厄方抗击疫情提供支持和帮助，同厄方开展形式多样的抗疫、疫苗合作。相信在双方共同努力下，中厄关系将得到更大发展。习近平强调，厄瓜多尔是中方共建“一带一路”的重要合作伙伴，双方在基础设施、能源矿产、金融等传统领域合作成果丰硕。中国经济进入新发展阶段，将更加开放、更具活力，这将为厄瓜多尔等国带来新的机遇。中方愿扩大自厄方进口规模，提高双方贸易和投资自由化便利化水平，培育健康、数字、绿色丝绸之路等新增长点，打造更多务实合作成果，更好造福两国人民。中方愿同厄方加强多边事务沟通协调，维护国际公平正义和发展中国家正当权益，推动构建人类命运共同体。拉索表示，我谨代表厄瓜多尔政府和人民再次热烈祝贺中国共产党成立100周年，祝贺中国共产党领导中国人民在包括摆脱贫困和抗击新冠肺炎疫情等问题上取得伟大成就。中方提供的疫苗为厄方抗击疫情提供了至关重要的支持，厄瓜多尔人民对此心怀感激，不会忘记。厄方反对将疫情政治化、污名化，希望同中方继续深化疫苗合作。厄方把中国视为最重要的全面战略伙伴，欢迎更多中方企业赴厄投资合作。',\n",
       " 'date': '2021-08-30',\n",
       " 'title': '习近平同厄瓜多尔总统拉索通电话',\n",
       " 'url': 'http://jhsjk.people.cn/article/32212337'}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016“中美旅游年”2月29日在京开幕',\n",
       " '国家主席习近平向开幕式致贺词',\n",
       " '习近平指出，共同举办2016“中美旅游年”是我去年9月对美国进行国事访问期间双方达成的一项重要成果',\n",
       " '值此2016“中美旅游年”开幕之际，我谨代表中国政府和人民，并以我个人的名义，对旅游年的开幕表示热烈的祝贺，对远道而来的美国朋友表示热烈的欢迎',\n",
       " '习近平指出，中美建交37年来，两国关系取得历史性发展',\n",
       " '近年来，双方共同努力推进中美新型大国关系建设，两国在诸多重要领域、重大国际和地区问题上开展了富有成效的协调与合作，不仅造福两国和两国人民，而且有力促进了世界和平、稳定、繁荣',\n",
       " '习近平表示，中美都有灿烂的文化、优美的风光，两国人民都有加深了解、增进友谊的强烈愿望',\n",
       " '希望双方以举办旅游年为契机，扩大人员往来，加强文化交流，为中美关系发展培育更为厚实的民意和社会基础',\n",
       " '欢迎更多美国朋友来中国旅游',\n",
       " '祝2016“中美旅游年”圆满成功']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016“中美旅游年”2月29日在京开幕。国家主席习近平向开幕式致贺词。习近平指出，共同举办2016“中美旅游年”是我去年9月对美国进行国事访问期间双方达成的一项重要成果。值此2016“中美旅游年”开幕之际，我谨代表中国政府和人民，并以我个人的名义，对旅游年的开幕表示热烈的祝贺，对远道而来的美国朋友表示热烈的欢迎。习近平指出，中美建交37年来，两国关系取得历史性发展。近年来，双方共同努力推进中美新型大国关系建设，两国在诸多重要领域、重大国际和地区问题上开展了富有成效的协调与合作，不仅造福两国和两国人民，而且有力促进了世界和平、稳定、繁荣。习近平表示，中美都有灿烂的文化、优美的风光，两国人民都有加深了解、增进友谊的强烈愿望。希望双方以举办旅游年为契机，扩大人员往来，加强文化交流，为中美关系发展培育更为厚实的民意和社会基础。欢迎更多美国朋友来中国旅游。祝2016“中美旅游年”圆满成功。(新华社北京3月1日电)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Jiaba cut\n",
    "result_cutted_sentence_in_string= []\n",
    "for string in sentence_list:\n",
    "    result_cutted_sentence_in_string.append(' '.join(list(jieba.cut(string))))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016 “ 中 美 旅游 年 ” 2 月 29 日 在京开幕',\n",
       " '国家 主席 习近平 向 开幕式 致 贺词',\n",
       " '习近平 指出 ， 共同 举办 2016 “ 中 美 旅游 年 ” 是 我 去年 9 月 对 美国 进行 国事访问 期间 双方 达成 的 一项 重要 成果',\n",
       " '值此 2016 “ 中 美 旅游 年 ” 开幕 之际 ， 我谨 代表 中国政府 和 人民 ， 并 以 我 个人 的 名义 ， 对 旅游 年 的 开幕 表示 热烈 的 祝贺 ， 对 远道而来 的 美国 朋友 表示 热烈 的 欢迎',\n",
       " '习近平 指出 ， 中 美 建交 37 年来 ， 两国关系 取得 历史性 发展',\n",
       " '近年来 ， 双方 共同努力 推进 中 美 新型 大国 关系 建设 ， 两国 在 诸多 重要 领域 、 重大 国际 和 地区 问题 上 开展 了 富有成效 的 协调 与 合作 ， 不仅 造福 两国 和 两国人民 ， 而且 有力 促进 了 世界 和平 、 稳定 、 繁荣',\n",
       " '习近平 表示 ， 中美 都 有 灿烂 的 文化 、 优美 的 风光 ， 两国人民 都 有 加深了解 、 增进友谊 的 强烈 愿望',\n",
       " '希望 双方 以 举办 旅游 年 为 契机 ， 扩大 人员 往来 ， 加强 文化交流 ， 为 中美关系 发展 培育 更为 厚实 的 民意 和 社会 基础',\n",
       " '欢迎 更 多 美国 朋友 来 中国 旅游',\n",
       " '祝 2016 “ 中 美 旅游 年 ” 圆满成功']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_cutted_sentence_in_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = textProcessor.embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.04141271,  0.53461641, -1.0505358 , ...,  0.06256261,\n",
       "         0.3142924 ,  2.01107216],\n",
       "       [-1.34495234,  0.692056  , -1.25638592, ...,  1.50753272,\n",
       "        -0.26766613,  1.0851773 ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SnowNLP to extract key words.\n",
    "s = SnowNLP(article_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = s.summary(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sentence_list_2 = []\n",
    "for sentence in tmp:\n",
    "    result_sentence_list_2.append(' '.join(list(jieba.cut(sentence))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016 “ 中 美 旅游 年 ” 2 月 29 日 在京开幕',\n",
       " '值此 2016 “ 中 美 旅游 年 ” 开幕 之际',\n",
       " '共同 举办 2016 “ 中 美 旅游 年 ” 是 我 去年 9 月 对 美国 进行 国事访问 期间 双方 达成 的 一项 重要 成果',\n",
       " '祝 2016 “ 中 美 旅游 年 ” 圆满成功',\n",
       " '不仅 造福 两国 和 两国人民']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_sentence_list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textProcessor.embedding[\"中国\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing_method(sentence_list: List[List[str]], embedding_size=200, a: float = 1e-3):\n",
    "    sentence_set = []\n",
    "    for sentence in sentence_list:  # sentence 是List[str]\n",
    "        vs = np.zeros(\n",
    "            embedding_size)  # add all word2vec values into one vector for the sentence\n",
    "        sentence_length = len(sentence)\n",
    "        for word in sentence:\n",
    "            a_value = a / (a + 0.0001)  # smooth inverse frequency, SIF\n",
    "            try:\n",
    "                word_embedding = textProcessor.embedding[word]\n",
    "#                 print(word_embedding)\n",
    "            except:\n",
    "                print(\"出错了\")\n",
    "                word_embedding = np.zeros(200)\n",
    "            vs = np.add(vs, np.multiply(a_value,word_embedding))  # vs += sif * word_vector\n",
    "        vs = np.divide(vs, sentence_length)  # weighted average\n",
    "        sentence_set.append(vs)  # add to our existing re-calculated set of sentences\n",
    "    \n",
    "    \n",
    "    # calculate PCA of this sentence set\n",
    "    pca = PCA()\n",
    "    pca.fit(np.array(sentence_set))\n",
    "    u = pca.components_[0]  # the PCA vector\n",
    "    u = np.multiply(u, np.transpose(u))  # u x uT\n",
    "\n",
    "    # pad the vector?  (occurs if we have less sentences than embeddings_size)\n",
    "    if len(u) < embedding_size:\n",
    "        for i in range(embedding_size - len(u)):\n",
    "            u = np.append(u, 0)  # add needed extension for multiplication below\n",
    "\n",
    "    # resulting sentence vectors, vs = vs -u x uT x vs\n",
    "    sentence_vecs = []\n",
    "    for vs in sentence_set:\n",
    "        sub = np.multiply(u, vs)\n",
    "        sentence_vecs.append(np.subtract(vs, sub))\n",
    "\n",
    "    return sentence_set,sentence_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = re.findall(\"(.*?)。\",dataset[\"articles\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = [list(jieba.cut(sentence)) for sentence in sentence_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_set,result_vecs = smoothing_method(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.random.rand(1,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp2 = np.multiply(tmp,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.10215674, 0.58870234, 1.93022113, 0.02695671, 0.1897214 ,\n",
       "        1.20312199, 1.33751979, 0.90356681, 1.32721979, 0.87632862,\n",
       "        1.10389802, 0.04345715, 0.25085491, 1.15381543, 1.75280503,\n",
       "        0.2425139 , 1.23929737, 0.03815315, 0.19768774, 0.06299306,\n",
       "        0.44798333, 0.24900866, 1.31624891, 1.85642033, 1.41833508,\n",
       "        0.00240756, 0.07121435, 0.41387025, 0.20759318, 1.35203002,\n",
       "        1.24788412, 0.48205375, 0.73628959, 1.8694331 , 0.95975569,\n",
       "        1.49139307, 0.00393641, 1.70571525, 0.57624458, 1.42027286,\n",
       "        0.45487374, 1.2799144 , 0.70637435, 0.25022228, 1.89248063,\n",
       "        1.40598964, 1.40236971, 0.06888766, 0.2004919 , 0.57732347,\n",
       "        1.35698993, 1.01505682, 1.82487082, 0.76975646, 0.70763003,\n",
       "        1.85111693, 0.75640849, 1.84858159, 1.41567508, 0.02115123,\n",
       "        1.21135467, 0.21978676, 0.46262773, 0.50462213, 1.67438286,\n",
       "        1.59746481, 1.79853788, 1.697656  , 1.12566811, 1.32884299,\n",
       "        1.13185954, 1.90157506, 1.22248099, 1.26854247, 1.75476968,\n",
       "        1.38988136, 0.13502981, 1.04989254, 0.87145143, 0.09534991,\n",
       "        1.2616776 , 1.36835095, 1.09562697, 0.96264019, 1.2499806 ,\n",
       "        1.10003462, 0.90265966, 1.08351815, 0.0968054 , 0.72712119,\n",
       "        0.17386713, 1.10419339, 0.24460271, 0.05180593, 0.56387563,\n",
       "        1.80563862, 1.66031322, 1.254371  , 0.88674978, 1.69811089,\n",
       "        0.59238338, 0.59775693, 1.28804452, 0.86422699, 0.03318376,\n",
       "        0.65129242, 0.48635057, 0.90919666, 1.67292857, 0.64617622,\n",
       "        0.19897203, 0.19980802, 0.45215451, 0.12996864, 0.38925603,\n",
       "        0.99134739, 0.53975337, 1.66448663, 0.82771788, 1.92061622,\n",
       "        0.49592948, 1.63273514, 1.79037737, 0.54419984, 0.32484198,\n",
       "        1.18931287, 0.41773828, 1.53692906, 0.10660685, 0.87123277,\n",
       "        1.07833821, 0.52664064, 1.24909068, 1.54977808, 1.02510211,\n",
       "        0.11124566, 1.43046767, 0.70062582, 1.64604693, 0.46574491,\n",
       "        1.5953038 , 1.22670049, 0.60857258, 0.63119024, 0.15894294,\n",
       "        1.69482683, 1.0332955 , 1.90201007, 0.56407796, 0.87039262,\n",
       "        1.82197786, 1.83840967, 0.28587496, 1.33990219, 0.53344306,\n",
       "        1.30563976, 1.52744191, 1.468824  , 1.3605577 , 0.21832534,\n",
       "        0.49211307, 1.93731393, 1.91946871, 1.41877397, 0.51163709,\n",
       "        0.81487786, 0.47264766, 0.93457478, 0.02142817, 0.90920138,\n",
       "        0.53123432, 1.10537396, 0.49995786, 0.65790831, 0.2595557 ,\n",
       "        1.70713586, 0.79094478, 1.60144018, 1.98644823, 0.95862832,\n",
       "        1.79978027, 0.36327861, 1.98692858, 1.48616203, 0.17273773,\n",
       "        0.57155641, 1.78840183, 0.41998585, 1.22089865, 1.40857143,\n",
       "        1.80561658, 0.14820947, 1.60898091, 1.60217358, 1.28082275,\n",
       "        1.25446713, 1.6900292 , 0.66122197, 1.42241746, 1.51076008]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
